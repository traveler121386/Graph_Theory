"""
ç½‘ç»œåˆ†ææ¨¡å—
è®¡ç®—ç½‘ç»œçš„åŸºæœ¬ç»“æ„ç‰¹æ€§å’Œä¸­å¿ƒæ€§æŒ‡æ ‡
"""

import networkx as nx
import pandas as pd
from typing import Dict, List, Tuple
import numpy as np


class NetworkAnalyzer:
    """ç½‘ç»œåˆ†æå™¨"""
    
    def __init__(self, G: nx.Graph):
        """
        åˆå§‹åŒ–åˆ†æå™¨
        
        Args:
            G: è¾“å…¥çš„ç½‘ç»œå›¾
        """
        self.G = G
        self.analysis_results = {}
    
    def calculate_basic_metrics(self) -> Dict:
        """
        è®¡ç®—ç½‘ç»œçš„åŸºæœ¬æŒ‡æ ‡
        
        Returns:
            åŒ…å«åŸºæœ¬æŒ‡æ ‡çš„å­—å…¸
        """
        print("\n" + "="*60)
        print("ğŸ“Š ç½‘ç»œåŸºæœ¬ç»“æ„åˆ†æ")
        print("="*60)
        
        metrics = {}
        
        # 1. èŠ‚ç‚¹æ•°å’Œè¾¹æ•°
        metrics['èŠ‚ç‚¹æ•°'] = self.G.number_of_nodes()
        metrics['è¾¹æ•°'] = self.G.number_of_edges()
        
        # 2. ç½‘ç»œå¯†åº¦
        metrics['ç½‘ç»œå¯†åº¦'] = nx.density(self.G)
        
        # 3. å¹³å‡åº¦
        degrees = [d for n, d in self.G.degree()]
        metrics['å¹³å‡åº¦'] = np.mean(degrees)
        metrics['æœ€å¤§åº¦'] = max(degrees)
        metrics['æœ€å°åº¦'] = min(degrees)
        
        # 4. èšç±»ç³»æ•°
        metrics['å¹³å‡èšç±»ç³»æ•°'] = nx.average_clustering(self.G)
        
        # 5. å¹³å‡æœ€çŸ­è·¯å¾„é•¿åº¦ï¼ˆä»…å¯¹è¿é€šå›¾ï¼‰
        if nx.is_connected(self.G):
            metrics['å¹³å‡æœ€çŸ­è·¯å¾„é•¿åº¦'] = nx.average_shortest_path_length(self.G)
            metrics['ç½‘ç»œç›´å¾„'] = nx.diameter(self.G)
        else:
            # å¯¹äºéè¿é€šå›¾ï¼Œè®¡ç®—æœ€å¤§è¿é€šåˆ†é‡çš„æŒ‡æ ‡
            largest_cc = max(nx.connected_components(self.G), key=len)
            G_largest = self.G.subgraph(largest_cc).copy()
            metrics['å¹³å‡æœ€çŸ­è·¯å¾„é•¿åº¦'] = nx.average_shortest_path_length(G_largest)
            metrics['ç½‘ç»œç›´å¾„'] = nx.diameter(G_largest)
            metrics['è¿é€šåˆ†é‡æ•°'] = nx.number_connected_components(self.G)
        
        # 6. åº¦åˆ†å¸ƒç»Ÿè®¡
        metrics['åº¦åˆ†å¸ƒ_å‡å€¼'] = np.mean(degrees)
        metrics['åº¦åˆ†å¸ƒ_ä¸­ä½æ•°'] = np.median(degrees)
        metrics['åº¦åˆ†å¸ƒ_æ ‡å‡†å·®'] = np.std(degrees)
        
        # æ‰“å°ç»“æœ
        print(f"\nåŸºæœ¬æŒ‡æ ‡:")
        print(f"  èŠ‚ç‚¹æ•°: {metrics['èŠ‚ç‚¹æ•°']}")
        print(f"  è¾¹æ•°: {metrics['è¾¹æ•°']}")
        print(f"  ç½‘ç»œå¯†åº¦: {metrics['ç½‘ç»œå¯†åº¦']:.4f}")
        print(f"  å¹³å‡åº¦: {metrics['å¹³å‡åº¦']:.2f}")
        print(f"  æœ€å¤§åº¦: {metrics['æœ€å¤§åº¦']}")
        print(f"  æœ€å°åº¦: {metrics['æœ€å°åº¦']}")
        print(f"  å¹³å‡èšç±»ç³»æ•°: {metrics['å¹³å‡èšç±»ç³»æ•°']:.4f}")
        print(f"  å¹³å‡æœ€çŸ­è·¯å¾„é•¿åº¦: {metrics['å¹³å‡æœ€çŸ­è·¯å¾„é•¿åº¦']:.2f}")
        print(f"  ç½‘ç»œç›´å¾„: {metrics['ç½‘ç»œç›´å¾„']}")
        
        self.analysis_results['basic_metrics'] = metrics
        return metrics
    
    def analyze_network_characteristics(self, metrics: Dict) -> str:
        """
        åˆ†æç½‘ç»œç‰¹æ€§çš„å«ä¹‰
        
        Args:
            metrics: åŸºæœ¬æŒ‡æ ‡å­—å…¸
        
        Returns:
            åˆ†æè¯´æ˜æ–‡æœ¬
        """
        print("\n" + "-"*60)
        print("ğŸ“ˆ ç½‘ç»œç‰¹æ€§åˆ†æè¯´æ˜")
        print("-"*60)
        
        analysis = []
        
        # å¯†åº¦åˆ†æ
        density = metrics['ç½‘ç»œå¯†åº¦']
        if density < 0.01:
            analysis.append(f"â€¢ ç½‘ç»œå¯†åº¦({density:.4f})è¾ƒä½ï¼Œè¯´æ˜ç½‘ç»œæ˜¯ç¨€ç–çš„ï¼Œå¤§å¤šæ•°ç”¨æˆ·ä¹‹é—´æ²¡æœ‰ç›´æ¥è¿æ¥")
        else:
            analysis.append(f"â€¢ ç½‘ç»œå¯†åº¦({density:.4f})ï¼Œè¯´æ˜ç½‘ç»œè¿æ¥ç¨‹åº¦ä¸­ç­‰")
        
        # èšç±»ç³»æ•°åˆ†æ
        clustering = metrics['å¹³å‡èšç±»ç³»æ•°']
        analysis.append(f"â€¢ å¹³å‡èšç±»ç³»æ•°({clustering:.4f})è¡¨ç¤ºç”¨æˆ·çš„æœ‹å‹åœˆä¸­ï¼Œæœ‹å‹ä¹‹é—´ä¹Ÿæœ‰è¿æ¥çš„æ¦‚ç‡")
        if clustering > 0.3:
            analysis.append("  è¿™è¡¨æ˜ç½‘ç»œä¸­å­˜åœ¨æ˜æ˜¾çš„ç¤¾å›¢ç»“æ„ï¼Œç”¨æˆ·å€¾å‘äºå½¢æˆç´§å¯†çš„å°åœˆå­")
        
        # å¹³å‡è·¯å¾„é•¿åº¦åˆ†æ
        avg_path = metrics['å¹³å‡æœ€çŸ­è·¯å¾„é•¿åº¦']
        analysis.append(f"â€¢ å¹³å‡æœ€çŸ­è·¯å¾„é•¿åº¦({avg_path:.2f})è¡¨ç¤ºä»»æ„ä¸¤ä¸ªç”¨æˆ·ä¹‹é—´çš„è·ç¦»")
        if avg_path < 10:
            analysis.append("  è¿™ä½“ç°äº†'å°ä¸–ç•Œ'ç‰¹æ€§ï¼Œå³ä½¿ç½‘ç»œå¾ˆå¤§ï¼Œä»»æ„ä¸¤ä¸ªç”¨æˆ·ä¹Ÿèƒ½é€šè¿‡è¾ƒå°‘çš„ä¸­é—´äººè¿æ¥")
        
        # åº¦åˆ†å¸ƒåˆ†æ
        analysis.append(f"â€¢ åº¦åˆ†å¸ƒçš„æ ‡å‡†å·®({metrics['åº¦åˆ†å¸ƒ_æ ‡å‡†å·®']:.2f})è¾ƒå¤§ï¼Œè¯´æ˜ç½‘ç»œä¸­å­˜åœ¨åº¦æ•°å·®å¼‚")
        analysis.append("  è¿™æ˜¯æ— æ ‡åº¦ç½‘ç»œçš„å…¸å‹ç‰¹å¾ï¼šå°‘æ•°hubèŠ‚ç‚¹è¿æ¥ä¼—å¤šç”¨æˆ·ï¼Œå¤§å¤šæ•°èŠ‚ç‚¹åº¦æ•°è¾ƒä½")
        
        result_text = "\n".join(analysis)
        print(result_text)
        
        return result_text
    
    def calculate_centrality_measures(self) -> pd.DataFrame:
        """
        è®¡ç®—å„ç§ä¸­å¿ƒæ€§æŒ‡æ ‡
        
        Returns:
            åŒ…å«ä¸­å¿ƒæ€§æŒ‡æ ‡çš„ DataFrame
        """
        print("\n" + "="*60)
        print("ğŸ¯ å…³é”®ç”¨æˆ·è¯†åˆ« - ç½‘ç»œä¸­å¿ƒæ€§åˆ†æ")
        print("="*60)
        
        # 1. åº¦ä¸­å¿ƒæ€§
        print("\nè®¡ç®—åº¦ä¸­å¿ƒæ€§...")
        degree_centrality = nx.degree_centrality(self.G)
        
        # 2. ä»‹æ•°ä¸­å¿ƒæ€§
        print("è®¡ç®—ä»‹æ•°ä¸­å¿ƒæ€§...")
        betweenness_centrality = nx.betweenness_centrality(self.G)
        
        # 3. æ¥è¿‘ä¸­å¿ƒæ€§
        print("è®¡ç®—æ¥è¿‘ä¸­å¿ƒæ€§...")
        closeness_centrality = nx.closeness_centrality(self.G)
        
        # 4. ç‰¹å¾å‘é‡ä¸­å¿ƒæ€§
        print("è®¡ç®—ç‰¹å¾å‘é‡ä¸­å¿ƒæ€§...")
        try:
            eigenvector_centrality = nx.eigenvector_centrality(self.G, max_iter=1000)
        except:
            eigenvector_centrality = {node: 0 for node in self.G.nodes()}
        
        # æ„å»º DataFrame
        centrality_df = pd.DataFrame({
            'ç”¨æˆ·': list(self.G.nodes()),
            'åº¦ä¸­å¿ƒæ€§': [degree_centrality[node] for node in self.G.nodes()],
            'ä»‹æ•°ä¸­å¿ƒæ€§': [betweenness_centrality[node] for node in self.G.nodes()],
            'æ¥è¿‘ä¸­å¿ƒæ€§': [closeness_centrality[node] for node in self.G.nodes()],
            'ç‰¹å¾å‘é‡ä¸­å¿ƒæ€§': [eigenvector_centrality[node] for node in self.G.nodes()],
        })
        
        # è®¡ç®—ç»¼åˆä¸­å¿ƒæ€§æ’å
        centrality_df['ç»¼åˆä¸­å¿ƒæ€§'] = (
            centrality_df['åº¦ä¸­å¿ƒæ€§'] / centrality_df['åº¦ä¸­å¿ƒæ€§'].max() * 0.3 +
            centrality_df['ä»‹æ•°ä¸­å¿ƒæ€§'] / centrality_df['ä»‹æ•°ä¸­å¿ƒæ€§'].max() * 0.3 +
            centrality_df['æ¥è¿‘ä¸­å¿ƒæ€§'] / centrality_df['æ¥è¿‘ä¸­å¿ƒæ€§'].max() * 0.2 +
            centrality_df['ç‰¹å¾å‘é‡ä¸­å¿ƒæ€§'] / centrality_df['ç‰¹å¾å‘é‡ä¸­å¿ƒæ€§'].max() * 0.2
        )
        
        # æ’åº
        centrality_df = centrality_df.sort_values('ç»¼åˆä¸­å¿ƒæ€§', ascending=False)
        
        self.analysis_results['centrality'] = centrality_df
        
        # æ‰“å°å‰10ä¸ªå…³é”®ç”¨æˆ·
        print("\nğŸŒŸ æ’åå‰10çš„å…³é”®ç”¨æˆ·:")
        print("-"*60)
        top_10 = centrality_df.head(10)
        for idx, row in top_10.iterrows():
            print(f"\n{idx+1}. {row['ç”¨æˆ·']}")
            print(f"   åº¦ä¸­å¿ƒæ€§: {row['åº¦ä¸­å¿ƒæ€§']:.4f} (è¿æ¥æ•°æœ€å¤š)")
            print(f"   ä»‹æ•°ä¸­å¿ƒæ€§: {row['ä»‹æ•°ä¸­å¿ƒæ€§']:.4f} (ä¿¡æ¯æµé€šæ¢çº½)")
            print(f"   æ¥è¿‘ä¸­å¿ƒæ€§: {row['æ¥è¿‘ä¸­å¿ƒæ€§']:.4f} (è·ç¦»å…¶ä»–ç”¨æˆ·æœ€è¿‘)")
            print(f"   ç»¼åˆæ’ååˆ†æ•°: {row['ç»¼åˆä¸­å¿ƒæ€§']:.4f}")
        
        return centrality_df
    
    def analyze_key_users(self, centrality_df: pd.DataFrame) -> str:
        """
        åˆ†æå…³é”®ç”¨æˆ·çš„ç‰¹å¾å’Œä½œç”¨
        
        Args:
            centrality_df: ä¸­å¿ƒæ€§æŒ‡æ ‡ DataFrame
        
        Returns:
            åˆ†æè¯´æ˜æ–‡æœ¬
        """
        print("\n" + "-"*60)
        print("ğŸ‘¥ å…³é”®ç”¨æˆ·ç‰¹å¾åˆ†æ")
        print("-"*60)
        
        analysis = []
        
        # åˆ†æåº¦ä¸­å¿ƒæ€§æœ€é«˜çš„ç”¨æˆ·
        top_degree = centrality_df.iloc[0]
        analysis.append(f"\nã€åº¦ä¸­å¿ƒæ€§æœ€é«˜çš„ç”¨æˆ·ã€‘")
        analysis.append(f"ç”¨æˆ·: {top_degree['ç”¨æˆ·']}")
        analysis.append(f"ç›´æ¥è¿æ¥æ•°: {int(top_degree['åº¦ä¸­å¿ƒæ€§'] * self.G.number_of_nodes())}")
        analysis.append(f"ä½œç”¨: è¿™ç±»ç”¨æˆ·æ˜¯'ç¤¾äº¤æ˜æ˜Ÿ'ï¼Œæ‹¥æœ‰æœ€å¤šçš„ç›´æ¥æœ‹å‹")
        
        # åˆ†æä»‹æ•°ä¸­å¿ƒæ€§æœ€é«˜çš„ç”¨æˆ·
        top_between = centrality_df.nlargest(1, 'ä»‹æ•°ä¸­å¿ƒæ€§').iloc[0]
        analysis.append(f"\nã€ä»‹æ•°ä¸­å¿ƒæ€§æœ€é«˜çš„ç”¨æˆ·ã€‘")
        analysis.append(f"ç”¨æˆ·: {top_between['ç”¨æˆ·']}")
        analysis.append(f"ä»‹æ•°ä¸­å¿ƒæ€§: {top_between['ä»‹æ•°ä¸­å¿ƒæ€§']:.4f}")
        analysis.append(f"ä½œç”¨: è¿™ç±»ç”¨æˆ·æ˜¯'ä¿¡æ¯æ¡¥æ¢'ï¼Œåœ¨ç½‘ç»œä¸­è¿æ¥ä¸åŒçš„ç¤¾åŒº")
        analysis.append(f"      ä»–ä»¬å¯¹ä¿¡æ¯ä¼ æ’­å’Œç½‘ç»œè¿é€šæ€§è‡³å…³é‡è¦")
        
        # åˆ†ææ¥è¿‘ä¸­å¿ƒæ€§æœ€é«˜çš„ç”¨æˆ·
        top_close = centrality_df.nlargest(1, 'æ¥è¿‘ä¸­å¿ƒæ€§').iloc[0]
        analysis.append(f"\nã€æ¥è¿‘ä¸­å¿ƒæ€§æœ€é«˜çš„ç”¨æˆ·ã€‘")
        analysis.append(f"ç”¨æˆ·: {top_close['ç”¨æˆ·']}")
        analysis.append(f"æ¥è¿‘ä¸­å¿ƒæ€§: {top_close['æ¥è¿‘ä¸­å¿ƒæ€§']:.4f}")
        analysis.append(f"ä½œç”¨: è¿™ç±»ç”¨æˆ·ä½äºç½‘ç»œçš„'ä¸­å¿ƒä½ç½®'ï¼Œèƒ½å¿«é€Ÿåˆ°è¾¾å…¶ä»–ç”¨æˆ·")
        
        result_text = "\n".join(analysis)
        print(result_text)
        
        return result_text
    
    def get_degree_distribution(self) -> Dict:
        """
        è·å–åº¦åˆ†å¸ƒä¿¡æ¯
        
        Returns:
            åº¦åˆ†å¸ƒå­—å…¸
        """
        degrees = [d for n, d in self.G.degree()]
        degree_counts = {}
        for d in degrees:
            degree_counts[d] = degree_counts.get(d, 0) + 1
        
        return degree_counts
    
    def run_all_analysis(self) -> Dict:
        """
        è¿è¡Œæ‰€æœ‰åˆ†æ
        
        Returns:
            åŒ…å«æ‰€æœ‰åˆ†æç»“æœçš„å­—å…¸
        """
        # åŸºæœ¬æŒ‡æ ‡
        metrics = self.calculate_basic_metrics()
        self.analyze_network_characteristics(metrics)
        
        # ä¸­å¿ƒæ€§åˆ†æ
        centrality_df = self.calculate_centrality_measures()
        self.analyze_key_users(centrality_df)
        
        return self.analysis_results


def main():
    """æµ‹è¯•ç½‘ç»œåˆ†ææ¨¡å—"""
    from data_generator import SocialNetworkGenerator
    
    # ç”Ÿæˆç½‘ç»œ
    generator = SocialNetworkGenerator(seed=42)
    G = generator.generate_complete_network(n_nodes=300, m=3)
    
    # åˆ†æç½‘ç»œ
    analyzer = NetworkAnalyzer(G)
    results = analyzer.run_all_analysis()
    
    print("\n" + "="*60)
    print("ç½‘ç»œåˆ†ææ¨¡å—æµ‹è¯•å®Œæˆ")
    print("="*60)


if __name__ == "__main__":
    main()

